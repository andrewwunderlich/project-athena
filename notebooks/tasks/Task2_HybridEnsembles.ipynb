{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from utils.model import load_pool, load_lenet\n",
    "from utils.file import load_from_json\n",
    "from utils.metrics import error_rate, get_corrections\n",
    "from models.athena import Ensemble, ENSEMBLE_STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "CONFIG_ROOT = \"../../src/configs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your candidate pools\n",
    "\n",
    "You can use individual json files for each type of models. The configuration should be similar to ``\"model-mnist.json\"``. \n",
    "\n",
    "Or, you can use a single json file for all types of models (the approach we used for this example) --- ``\"hybrid-mnist.json\"``.\n",
    "\n",
    "Sample configuration for specific type of model:\n",
    "\n",
    "e.g., example for ``cnn`` models:\n",
    "```json\n",
    "\"cnn\": {\n",
    "    \"dir\": \"../../models/cnn/\",\n",
    "    \"um_file\": \"model-mnist-cnn-clean.h5\",\n",
    "    \"wd_prefix\": \"model-mnist-cnn-\",\n",
    "    \"wd_postfix\": \".h5\",\n",
    "    \"architecture\": \"cnn\"\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "* python example: `tutorials/eval_model.py`\n",
    "* main api: `tutorials.eval_model.evaluate_hybrid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from tutorials/eval_model.py\n",
    "def evaluate_hybrid(trans_configs, model_configs,\n",
    "                    data_configs, save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Apply transformation(s) on images.\n",
    "    :param trans_configs: dictionary. The collection of the parameterized transformations to test.\n",
    "        in the form of\n",
    "        { configsx: {\n",
    "            param: value,\n",
    "            }\n",
    "        }\n",
    "        The key of a configuration is 'configs'x, where 'x' is the id of corresponding weak defense.\n",
    "    :param model_configs:  dictionary. Defines model related information.\n",
    "        Such as, location, the undefended model, the file format, etc.\n",
    "    :param data_configs: dictionary. Defines data related information.\n",
    "        Such as, location, the file for the true labels, the file for the benign samples,\n",
    "        the files for the adversarial examples, etc.\n",
    "    :param save: boolean. Save the transformed sample or not.\n",
    "    :param output_dir: path or str. The location to store the transformed samples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the baseline defense (PGD-ADT model)\n",
    "    baseline = load_lenet(file=model_configs.get('pgd_trained'), trans_configs=None,\n",
    "                                  use_logits=False, wrap=False)\n",
    "\n",
    "    # get the CNN undefended model (UM)\n",
    "    # Since the uploaded AEs were generated for the CNN UM,\n",
    "    # collect the corrections based on the CNN UM.\n",
    "    cnn_configs = model_configs.get('cnn')\n",
    "    file = os.path.join(cnn_configs.get('dir'), cnn_configs.get('um_file'))\n",
    "    undefended = load_lenet(file=file,\n",
    "                            trans_configs=trans_configs.get('configs0'),\n",
    "                            wrap=True)\n",
    "    print(\">>> um:\", type(undefended))\n",
    "\n",
    "    # load CNN weak defenses into a pool\n",
    "    # tiny pool: 3 weak defenses\n",
    "    cnn_pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                            model_configs=cnn_configs,\n",
    "                            active_list=True,\n",
    "                            wrap=True)\n",
    "\n",
    "    cnns = list(cnn_pool.values())\n",
    "    print(\">>> CNNs:\", type(cnns), type(cnns[0]))\n",
    "\n",
    "    # load SVM weak defenses into a pool\n",
    "    # tiny pool: 3 weak defenses\n",
    "    svm_configs = model_configs.get('svm')\n",
    "    svm_pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                            model_configs=svm_configs,\n",
    "                            active_list=True,\n",
    "                            wrap=True)\n",
    "\n",
    "    svms = list(svm_pool.values())\n",
    "    print(\">>> SVMs:\", type(svms), type(svms[0]))\n",
    "\n",
    "    # ----------------------\n",
    "    # Hybrid\n",
    "    # ----------------------\n",
    "    \"\"\"\n",
    "    There are multiple ways to select weak defenses for your ensemble.\n",
    "    I shew you a naive strategy here.\n",
    "    Try to come up your own strategy, so you will create several variants of\n",
    "    hybrid ensembles. Evaluate all your ensemble variants.\n",
    "    \n",
    "    For examlpe, here is a naive strategy to build N hybrid variants:\n",
    "    1. fix the number of total weak defenses in the hybrid ensemble. (e.g., 50)\n",
    "    2. build a hybrid ensemble, starting from all CNN weak defenses. (50 cnns, 0 svms) -- h1\n",
    "    3. build your next hybrid ensemble by replacing part of the CNN weak defenses with SVM weak defenses.\n",
    "       For examples, replace 10%, then 20%, 30%, etc. of the weak defenses with SVMs,\n",
    "       until you get a hybrid consisting only SVMs. \n",
    "       (45 cnns, 5 svms) -- h2; \n",
    "       (40 cnns, 10 svms) -- h3; \n",
    "       ...; \n",
    "       (0 cnns, 50 svms) -- hN.\n",
    "    4. So, you will have 11 ensemble variants, varying the ratio between two types of models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select weak defenses for your ensemble.\n",
    "    # In this example, we build a hybrid ensemble consisting of 3 cnn models and 3 svm models\n",
    "    wds = cnns\n",
    "    wds.extend(svms)\n",
    "    # create the ensemble\n",
    "    ensemble = Ensemble(classifiers=wds,\n",
    "                        strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Evaluate models\n",
    "    # -----------------------------\n",
    "    # load the benign samples\n",
    "    bs_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "    x_bs = np.load(bs_file)\n",
    "    img_rows, img_cols = x_bs.shape[1], x_bs.shape[2]\n",
    "\n",
    "    # load the corresponding true labels\n",
    "    label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "    labels = np.load(label_file)\n",
    "\n",
    "    # get indices of benign samples that are correctly classified by the targeted model\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(bs_file))\n",
    "    pred_bs = undefended.predict(x_bs)\n",
    "    corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "    # Evaluate AEs.\n",
    "    results = {}\n",
    "    ae_list = data_configs.get('ae_files')\n",
    "    ae_file = os.path.join(data_configs.get('dir'), ae_list[4])\n",
    "    x_adv = np.load(ae_file)\n",
    "\n",
    "    # evaluate the undefended model on the AE\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_um = undefended.predict(x_adv)\n",
    "    err_um = error_rate(y_pred=pred_adv_um, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    results['UM'] = err_um\n",
    "\n",
    "    # evaluate the ensemble on the AE\n",
    "    # if you have multiple ensemble variants, you need to perform this evaluation\n",
    "    # for each of the ensembles.\n",
    "    print(\">>> Evaluating ensemble on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_ens = ensemble.predict(x_adv)\n",
    "    err_ens = error_rate(y_pred=pred_adv_ens, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    # you may want to use a different key here, in order to distinguish the ensemble variants.\n",
    "    results['Ensemble'] = err_ens\n",
    "\n",
    "    # evaluate the baseline on the AE\n",
    "    print(\">>> Evaluating baseline model on [{}], it may take a while...\".format(ae_file))\n",
    "    pred_adv_bl = baseline.predict(x_adv)\n",
    "    err_bl = error_rate(y_pred=pred_adv_bl, y_true=labels, correct_on_bs=corrections)\n",
    "    # track the result\n",
    "    results['PGD-ADT'] = err_bl\n",
    "\n",
    "    # TODO: collect and dump the evaluation results to file(s) such that you can analyze them later.\n",
    "    print(\">>> Evaluations on [{}]:\\n{}\".format(ae_file, results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading model [../../models/baseline/advTrained-mnist-adtC.h5]...\n",
      "WARNING:tensorflow:From /home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/meng/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      ">>> Loading model [../../models/cnn/model-mnist-cnn-clean.h5]...\n",
      ">>> um: <class 'models.keraswrapper.WeakDefense'>\n",
      ">>> Loading model [../../models/cnn/model-mnist-cnn-shift_bottom_left.h5]...\n",
      ">>> Loading model [../../models/cnn/model-mnist-cnn-affine_both_stretch.h5]...\n",
      ">>> Loading model [../../models/cnn/model-mnist-cnn-cartoon_mean_type3.h5]...\n",
      ">>> Loaded 3 models.\n",
      ">>> CNNs: <class 'list'> <class 'models.keraswrapper.WeakDefense'>\n",
      ">>> Loading model [../../models/svm/model-mnist-svm-shift_bottom_left.pkl]...\n",
      ">>> Loading model [../../models/svm/model-mnist-svm-affine_both_stretch.pkl]...\n",
      ">>> Loading model [../../models/svm/model-mnist-svm-cartoon_mean_type3.pkl]...\n",
      ">>> Loaded 3 models.\n",
      ">>> SVMs: <class 'list'> <class 'models.sklearnwrapper.ScikitlearnSVC'>\n",
      ">>> Evaluating UM on [../../data/test_BS-mnist-clean.npy], it may take a while...\n",
      ">>> Evaluating UM on [../../data/test_AE-mnist-cnn-clean-fgsm_eps0.3.npy], it may take a while...\n",
      ">>> Evaluating ensemble on [../../data/test_AE-mnist-cnn-clean-fgsm_eps0.3.npy], it may take a while...\n",
      ">>> Evaluating baseline model on [../../data/test_AE-mnist-cnn-clean-fgsm_eps0.3.npy], it may take a while...\n",
      ">>> Evaluations on [../../data/test_AE-mnist-cnn-clean-fgsm_eps0.3.npy]:\n",
      "{'UM': 0.8842541157458843, 'Ensemble': 0.39723260276739725, 'PGD-ADT': 0.21331178668821332}\n"
     ]
    }
   ],
   "source": [
    "# load experiment configurations\n",
    "trans_configs = load_from_json(\"../../src/configs/demo/athena-mnist.json\")\n",
    "model_configs = load_from_json(\"../../src/configs/demo/hybrid-mnist.json\")\n",
    "data_configs = load_from_json(\"../../src/configs/demo/data-mnist.json\")\n",
    "\n",
    "output_dir = \"../../results\"\n",
    "\n",
    "# -------- Evaluate HYBRID ATHENA -------------\n",
    "evaluate_hybrid(trans_configs=trans_configs,\n",
    "                model_configs=model_configs,\n",
    "                data_configs=data_configs,\n",
    "                save=False,\n",
    "                output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
